---
title: "TD4 - Détection de nouveauté par One-class SVM et Kernel PCA"
author: "Romain Dudoit, Franck Doronzo, Marie Vachet"
date: "01/01/2022"
output:
  bookdown::pdf_document2:
    latex_engine: lualatex
  tufte::tufte_handout:
    latex_engine: xelatex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Présentation de l'étude de cas

Nous nous intéressons à des données sur le cancer du sein, recueillies à l’hôpital de l’université de Wisconsin par le Dr. Wolberg. Cette étude de cas à pour objectif la détection des tumeurs malignes à partir de différentes variables explicatives numériques. Les données médicales fournies sont déséquilibrées avec 2/3 de cas bénins. Cette particularité nous pousse à utiliser des méthodes de détection de nouveauté : le One Class SVM et le kernel PCA. L’objectif de la détection de nouveauté est d’identifier l’appartenance d’un objet `a une classe spécifique parmi plusieurs autres classes et dont les observations sont relativement rares. Dans notre cas, la classe que l’on souhaite modéliser est celle des tumeurs bégnines (puisqu’il s’agit de la classe majoritaire). Une observation maligne sera celle qui ne rentre pas dans la classe “classique” et elle est donc nouvelle.
Cette étude est tirée de l’article suivant : Hoffmann, H. (2007). Kernel PCA for novelty detection. Pattern Recognition, 40(3), 863-874

#	Les méthodes utilisées

One class SVM :  Le but de cette méthode, basée sur les Support Vector Machine, est de séparer les points de la classe majoritaire de l’origine de l’espace : on cherche l’hyperplan le plus éloigné de l’origine qui sépare les données de l’origine. Pour cela, la technique cherche à maximiser la distance (la marge) entre cette frontière linéaire et l’origine.

Kernel PCA : Cette fois-ci nous utilisons une méthode non supervisée de détection d'anomalies: l'Analyse en Composantes Principales (ACP) à noyau. Afin de classer les données de test, nous nous basons sur un score appelé “reconstruction error” (erreur de reconstruction) : il s'agit de la distance entre le point à tester, représenté dans un espace de redescription, et sa projection sur l'espace généré par l'ACP à noyau sur un échantillon d'appentissage.

# Réponses aux questions 


## Lecture et description des données

Question 1:

Question 2:
```{r}
D = read.table("breast-cancer-wisconsin.data",sep = ",", na.strings = "?")
colnames(D) = c("code_number", "clump_thickness", "cell_size_uni", "cell_shape_uni", "marginal_adhesion", "single_epithetial_cell_size", "bare_nuclei", "bland_chromatin", "normal_nucleoli", "mitoses", "class")

```
La commande na.strings permet de préciser quelles valeurs concidérer comme na.

Question 3:
```{r}
print(class(D))
print(str(D))
```
La commande 'class' nous indique que D est un objet de type data.frame.
La commande 'str' nous donne une synthèse des données avec le type et un extrait de chaque colonnes. De plus, on notera que nous étudierons 699 cas et 11 variables. La dernière variable est la variable cible à prédire.

```{r}
print(head(D))
```

La commande 'head' affiche les premières lignes de D.

```{r}
summary(D)
```
La commande 'summary' nous affiche quelques indicateurs statistiques comme la mediane et la moyenne des colonnes.

## Séparation des donneés en "train" et "test"

Question 4: 

La variable D comporte des données manquantes. Identifiez les observations comportant au moins une donnée manquante à l’aide de la commande complete.cases. Vous devez identifier 16 cas.

```{r}
na_row = complete.cases(D)[complete.cases(D)==FALSE]  
length(na_row) 
```
La variable na_row contient un vecteur de booleéns des observations contenant des valeurs manquantes. On a bien 16 observations non completes.

Question 5:

```{r}
D = D[complete.cases(D),]
nrow(D)
```
On obtient donc 699-16 = 683 lignes

Question 6: 

```{r}
X = as.matrix(D[, 2:10]) 
y = D$class
```

On stocke dans X les données des colonnes 2 à 10 et dans y la variable cible (class)

Question 7: 

```{r}
length(y[y==2]) 
length(y[y==4])
y<-factor(y)
levels(y) <- c(0,1)
length(y[y==0]) 
length(y[y==1])
```
En transformant y en factor, il devient facile de changer ces niveaux en 0 et 1.

Quesion 8: 

```{r}
benin = which(y == 0, arr.ind = TRUE)
length(benin)
malin = which(y == 1, arr.ind = TRUE)
length(malin)
```

Question 9:

```{r}
train_set = benin[1:200]
test_set = -benin[1:200] 
```
Le - devant benin[1:200] pour définir l'échantillon de test permet de ne pas sélectionner les indices des 200 premières valeurs. Les données d'entrainement sont donc toutes bénines alors que les données de test sont mixtes. L'échantillon de test est composé de 483 individus.

## One-class SVM

Question 10:

```{r include=FALSE}
library(e1071)
```

Question 11:

```{r}
oc_svm_fit = svm(as.matrix(X[train_set,]), type = "one-classification", gamma = 1/2)
oc_svm_fit
```
Notre modèle a 106 support vectors, c'est à dire 106 observations servant à la définition de l'hyperplan.

Question 12: 

```{r}
oc_svm_pred_test = predict(oc_svm_fit, newdata = X[test_set,], decision.values = TRUE)
str(oc_svm_pred_test) 

```
On applique le modèle entrainé sur les données de test. 
La commande str permet de voir les attributs de l'objet oc_svm_pred_test: 

Question 13:

```{r}
head(attr(oc_svm_pred_test, "decision.values"))
```
On obtient les scores d'appartenance à la classe majoritaire de chaque observation.

```{r}
oc_svm_score_test = -as.numeric(attr(oc_svm_pred_test ,"decision.values")) 
head(oc_svm_score_test)
```
Le signe et le type des prédictions ont été modifiés.


## Courbe ROC

Question 14:

```{r include=FALSE}
library(ROCR)
```

Question 15: 

```{r}
pred_oc_svm = prediction(oc_svm_score_test, y[test_set])
```
La commande 'prediction' transforme le modele en un objet de la classe utilisée par les fonctions du package ROCR.

```{r}
oc_svm_roc = performance(pred_oc_svm, measure = "tpr", x.measure = "fpr")
str(oc_svm_roc)
head(oc_svm_roc@alpha.values[[1]])
tail(oc_svm_roc@alpha.values[[1]])
```
On obtient un obet de classe performance composé des mesures testées (taux de vrais positifs et le taux de faux positifs) pour diffé©rentes valeurs de alpha (de Inf à -3.501672)

```{r}
plot(oc_svm_roc)
```
Le graphique ci-dessus représente la courbe ROC du modèle que nous venons de créer. 

Question 16:

Il semblerait que le classifieur créé soit assez performant puisque, au premier coup d'oeil, il est au dessus du classifieur aléatoire (une droite diagonale au repère) et, plus précisément, son meilleur 'point' est assez proche du point idéal (0,1).

Pour aller plus loin, nous calculons son aire sous la courbe (AUC) :

```{r}
oc_svm_auc <- performance(pred_oc_svm, "auc")
oc_svm_auc@y.values[[1]]
```

L'aire sour la courbe (AUC) est de 0.993, ce qui est tres proche de 1 (AUC du meilleur modele possible). Notre modele est donc très bon.

## Kernel PCA

Question 17: 

```{r include=FALSE}
library(kernlab)
kernel=rbfdot(sigma=1/8)
Ktrain=kernelMatrix(kernel,as.matrix(X[train_set,]))
```
Nous procédons à la création d'un noyau gaussien: c'est un objet de type 'kernel'.
A la suite de quoi nous appliquons ce noyau sur les données d'apprentissage afin de créer la  matrice K (Ktrain). Ces lignes de commandes implémente cette partie du sujet : 
" Projeter implicitement les observations d’entraˆınement dans F en calculant la matrice `a noyaux K de terme général K(xi, xj) = <Φ(xi),  Φ(xj)>, ∀i, j = 1, . . . , n.

Question 18:

```{r}
k2=apply(Ktrain,1,sum) 
k3=apply(Ktrain,2,sum) 
k4=sum(Ktrain)
n=nrow(Ktrain)
KtrainCent=matrix(0,ncol=n,nrow=n)
```
Dans la formule 1, k2 représente la somme en ligne, k3 la somme en colonnes et k4 la somme globale.
KtrainCent est initialisée avec des 0.

```{r}
for (i in 1:n)
  {
    for (j in 1:n)
      {
        KtrainCent[i,j]=Ktrain[i,j]-1/n*k2[i]-1/n*k3[j]+1/n^2*k4
      }
}
```

Le code dans cette boucle transforme la matrice noyau K en un produit scalaire des vecteurs centres dans F, en utilisant la formule 1.

Question 19:

```{r}
eigen_KtrainCent = eigen(KtrainCent)
str(eigen_KtrainCent)
```
On effectue notre ACP sur la matrice KtrainCent.

Question 20:  

```{r}
s = 80
A=eigen_KtrainCent$vectors[,1:s]%*%diag(1/sqrt(
  eigen_KtrainCent$values[1:s]))
dim(A)
```
Nous gardons les 80 permiers axes principaux.
Ces lignes de code implémentent les coefficients alpha, définis par cette formule : αm = 1 /(√λ^m) * v^m

Question 21:

```{r}
K=kernelMatrix(kernel,X)
dim(K)
```

La matrice K ainsi définie correspond à la projection dans l'espace F de toutes les données.

Question 22: 

```{r}
n=683
p1=as.numeric(diag(K[test_set,test_set]))
p2 =apply(K[test_set,train_set],1,sum)
p3=sum(Ktrain)
```

On calcule le carre de la distance euclidienne entre l'origine et le vecteur dans F, dont la formule est la 4.
La variable p1 correspond à k(z,z)
La variable p2 correspond à la somme en ligne de K(z, xi)
La variable p3 correspond à la double somme de k(xi, xj)

Question 23: 

```{r}
ps=p1-(2/n *p2)+(1/n^2 *p3)
length(ps)
```

Question 24:

```{r}
f1 = K[test_set,train_set]

f2 = apply(K[train_set,train_set], 1, sum)

f3 = apply(K[test_set,train_set], 1, sum)

f4 = sum(K[train_set,train_set])
```

Nous cherchons à obtenir f: les vecteurs comportant les coordonnées des vecteur Φ˜(z) − Φ˜(0) sur chaque axe pour tout z appartenant à l’ensemble de test.
On retrouve f1 : K(z, xi), f2 : somme en ligne de K(xi, xr), f3 : somme en ligne de K(z, xr), f4 : double somme K(xr, xs). 

f4 a déjà été calculé précédemment: il s'agit de k4 et de p3.

Question 25: 

```{r}
intermediaire = f1 - (1/n * f2) - (1/n * f3) + (1/n^2 * f4)
dim(intermediaire)
```
Nous obtenons ainsi la somme entre les grandes parenthèses de la formule 5.

```{r}
f = intermediaire %*% A
dim(f)
```
La matrice f est de bonnes dimensions (le nombre de lignes vaut le nombre de données tests et le nombre de colonne, le nombre d’axes principaux retenus).

Question 26: 

Le score défini en (3) est composé de la différence entre 2 éléments :
— le carré de la distance euclidienne entre l’origine et le vecteur dans F,
— le carré de la distance euclidienne entre l’origine et le vecteur dans le sous-espace réduit de F obtenu par l’ACP à noyaux.
Nous avons déjà calculé le premier terme, calculons le second à l'aide de la formule n°6.

```{r}
eq6 = apply(f^2,1,sum)
str(eq6)
```
A présent, nous pouvons calculer le score de "reconstruction error”: 

```{r}
kpca_score_test = ps - eq6
head(kpca_score_test)
```
Plus ce score est grand, plus z est un point nouveau.

Question 27:

Afin d'évaluer notre modèle, nous allons à nouveau construire la courbe ROC.
```{r}
pred_oc_kpca = prediction(kpca_score_test,y[test_set])
oc_kpca_roc = performance(pred_oc_kpca, measure = "tpr", x.measure= "fpr")
plot(oc_kpca_roc)
```
L'allure de cette courbe semble indiquer que notre modèle "kernel PCA" est plutôt bon.
Comparons les deux modèles que nous avons utilisés.

```{r}
plot(oc_svm_roc)
plot(oc_kpca_roc, add=TRUE, col=2)
legend(x = "bottomright", 
       legend = c("One-class SVM", "ACP Kernel"),
       fill = 1:2)
```
On observe que les deux courbes sont très proches mais le OneClass SVM semble plus performant. Pour vérifier cette intuition, calculons l'aire sous la courbe.


```{r}
auc_ROCR <- performance(pred_oc_kpca, measure = "auc")
auc_ROCR <- auc_ROCR@y.values[[1]]
print(auc_ROCR)
```
Nous obtenons une AUC de 0.987 ce qui est très performant mais un peu moins que pour la première méthode où nous obtenions 0.993.

#	Discussion et comparaison des modèles

Si nous devions choisir un des deux modèles construits, nous prendrions le premier. En effet, ses performances pour détecter les individus 'nouveaux' c'est à dire les tumeurs malignes, sont légèrement meilleures que l'ACP à noyau. L'important à retenir étant quand même que la méthode non supervisée est presque aussi efficace que la méthode supervisée dans ce cas.
